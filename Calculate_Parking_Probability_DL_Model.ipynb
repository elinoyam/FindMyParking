{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Calculate Parking Probability DL Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxqw5uvUhUJFZhfpubuMTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elinoyam/FindMyParking/blob/WorkingWithColab/Calculate_Parking_Probability_DL_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "HVLptjAF3t7L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want a model that get the number of free parking spots and occupied parking spots in a street from the bashini project and predicts the probability to find an empty parking spot in the street."
      ],
      "metadata": {
        "id": "ghogIo-V50dY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KHfwB8Kh25xS"
      },
      "outputs": [],
      "source": [
        "# linear regression model\n",
        "# Create the model class\n",
        "class ParkingProbModel(torch.nn.Module):\n",
        "    def __init__(self,input_dim,output_dim): \n",
        "        super(ParkingProbModel, self).__init__()\n",
        "        # One input and one output \n",
        "        self.linear = torch.nn.Linear(input_dim,output_dim)    \n",
        "\n",
        "    def forward(self, x): \n",
        "        prediction = self.linear(x) \n",
        "        return prediction "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our data is in the form: [...[free_parkings,occupied_parkings,probability]...]\n",
        "# need to be transform to:\n",
        "  # input_data = [...[free_parkings,occupied_parkings]...]\n",
        "  # targets = [...,[probability],...]\n",
        "\n",
        "def get_transformed_datasets(original_data):\n",
        "  data = torch.tensor(original_data) # maybe needs to be torch.from_numpy()\n",
        "  input_data = data[:,:2]\n",
        "  labels = data[:,2:]\n",
        "  \n",
        "  return input_data, labels"
      ],
      "metadata": {
        "id": "f-VyT2hy6OgD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming arr is the data we want the model to learn from\n",
        "arr = [[1,2,3],[4,5,6],[7,8,9]]\n",
        "validation_ratio = 0.2\n",
        "batch_size = 16\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "epoch = 500\n",
        "\n",
        "input_data, labels = get_transformed_datasets(arr)\n",
        "\n",
        "dataset = TensorDataset(input_data, labels)\n",
        "\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=validation_ratio, shuffle=True)\n",
        "\n",
        "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "dl_valid = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initializing the model\n",
        "model = ParkingProbModel(input_dim, output_dim)\n",
        "# Define the loss\n",
        "loss_function = nn.CrossEntropyLoss()  \n",
        "lr = 0.01 # if not good try the original lr: 0.001\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizers = torch.optim.SGD(model.parameters(), lr=lr)  "
      ],
      "metadata": {
        "id": "stwo8yLE6h7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batchsiz = 100\n",
        "# niter = 2000\n",
        "# numepoch = niter / (len(trainds) / batchsiz)\n",
        "# numepoch = int(numepoch)\n",
        "\n",
        "# Train the model\n",
        "iters = 0\n",
        "for epoch in range(epoch):\n",
        "    for x, (params, labels) in enumerate(dl_train):\n",
        "        optimizers.zero_grad() # clear the gradient\n",
        "        \n",
        "        prediction = model(params) # Forward pass to get output\n",
        "\n",
        "        losses = loss_function(prediction, labels)\n",
        "\n",
        "        losses.backward()\n",
        "\n",
        "        optimizers.step()\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "        if iters % 50 == 0: # check status\n",
        "            corrects = 0\n",
        "            totals = 0\n",
        "            \n",
        "            for inputs, lbls in dl_valid:\n",
        "                outps = model(inputs)\n",
        "\n",
        "                _, predict = torch.max(outps.data, 1)\n",
        "\n",
        "                totals += lbls.size(0)\n",
        "                corrects += (predict == lbls).sum() # total number of correct prediction.\n",
        "\n",
        "            accuracy = 100 * corrects / totals\n",
        "\n",
        "            print('Iterations: {}. Loss: {}. Accuracy: {}'.format(iters, losses.item(), accuracy))\n"
      ],
      "metadata": {
        "id": "qGLuzAUR8AEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}